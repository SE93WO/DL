{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "columns_to_parse = ['device','geoNetwork','totals','trafficSource']\n",
    "\n",
    "def parse_dataframe(path):\n",
    "    data_df= pd.read_csv(path,converters={column: json.loads for column in columns_to_parse},\n",
    "                                dtype={'fullVisitorId':str})\n",
    "    \n",
    "    for col in columns_to_parse:\n",
    "        json_col_df = json_normalize(data_df[col])\n",
    "        json_col_df.columns = [f\"{col}_{sub_col}\" for sub_col in json_col_df.columns]\n",
    "        data_df = data_df.drop(col,axis=1).merge(json_col_df,right_index=True,left_index=True)\n",
    "        \n",
    "    return data_df \n",
    "def process_datetime(data_df):\n",
    "    data_df['date'] = data_df['date'].astype(str)\n",
    "    data_df['date'] = data_df['date'].apply(lambda x:x[:4]+\"-\"+x[4:6]+\"-\"+x[6:])\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    data_df['year'] = data_df['date'].dt.year\n",
    "    data_df['month'] = data_df['date'].dt.month\n",
    "    data_df['day'] = data_df['date'].dt.day\n",
    "    data_df['weekday'] =data_df['date'].dt.weekday\n",
    "    data_df['weekofyear'] =data_df['date'].dt.weekofyear\n",
    "    data_df['month_unique_user_count'] = data_df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    data_df['day_unique_user_count'] = data_df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    data_df['weekday_unique_user_count'] = data_df.groupby('weekday')['fullVisitorId'].transform('nunique')\n",
    "    return data_df\n",
    "def process_format(data_df):\n",
    "    print(\"Inside process_format function\")\n",
    "    for col in ['visitNumber','totals_hits','totals_pageviews']:\n",
    "        data_df[col] = data_df[col].astype(float)\n",
    "            \n",
    "    data_df['trafficSource_adwordsClickInfo.isVideoAd'].fillna(True,inplace=True)\n",
    "    data_df['trafficSource_isTrueDirect'].fillna('False',inplace=True)\n",
    "    return data_df\n",
    "def process_device(data_df):\n",
    "    print(\"process device\")\n",
    "    data_df['browser_category'] = data_df['device_browser'] + \"_\"+data_df['device_deviceCategory']\n",
    "    data_df['browser_os'] = data_df['device_browser'] + \"_\"+data_df['device_operatingSystem']\n",
    "    return data_df\n",
    "def process_totals(data_df):\n",
    "    print(\"process totals..\")\n",
    "    data_df['visitNumber'] = np.log1p(data_df['visitNumber'])\n",
    "    data_df['totals_hits'] = np.log1p(data_df['totals_hits'])\n",
    "    data_df['totals_pageviews'] = np.log1p(data_df['totals_pageviews']).fillna(0)\n",
    "    data_df['mean_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('mean')\n",
    "    data_df['sum_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('sum')\n",
    "    data_df['max_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('max')\n",
    "    data_df['min_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('min')\n",
    "    data_df['var_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('var')\n",
    "    data_df['mean_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('mean')\n",
    "    data_df['sum_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('sum')\n",
    "    data_df['max_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('max')\n",
    "    data_df['min_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('min')\n",
    "    return data_df\n",
    "def process_geo_network(data_df):\n",
    "    print(\"process geo network...\")\n",
    "    data_df['sum_pageviews_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_pageviews'].transform('sum')\n",
    "    data_df['count_pageviews_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_pageviews'].transform('count') \n",
    "    data_df['mean_pageviews_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_pageviews'].transform('mean')\n",
    "    data_df['sum_hits_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_hits'].transform('sum')\n",
    "    data_df['count_hits_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_hits'].transform('count')\n",
    "    data_df['mean_hits_per_network_domain'] = data_df.groupby(['geoNetwork_networkDomain'])['totals_hits'].transform('mean')\n",
    "    return data_df                                                  \n",
    "pd.set_option('display.max.columns',None)\n",
    "def process_traffic_source(data_df):\n",
    "    print(\"process traffic source....\")\n",
    "    data_df['source_country'] = data_df['trafficSource_source']+\"_\" +data_df['geoNetwork_country']\n",
    "    data_df['campaign_medium'] = data_df['trafficSource_campaign']+\"_\" +data_df['trafficSource_medium']\n",
    "    data_df['medium_hits_mean'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('mean')\n",
    "    data_df['medium_hits_min'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('min')\n",
    "    data_df['medium_hits_max'] =data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('max')\n",
    "    data_df['medium_hits_sum'] = data_df.groupby(['trafficSource_medium'])['totals_hits'].transform('sum')\n",
    "    return data_df\n",
    "g_train_df = parse_dataframe('../input/train.csv')\n",
    "g_train_df = process_datetime(g_train_df)\n",
    "g_test_df = parse_dataframe('../input/test.csv')\n",
    "g_test_df = process_datetime(g_test_df) \n",
    "\n",
    "cols_to_drop = [col for col in g_train_df.columns if g_train_df[col].nunique(dropna=False) == 1]\n",
    "g_train_df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "g_test_df.drop([col for col in cols_to_drop if col in g_test_df.columns], axis=1, inplace=True)\n",
    "\n",
    "g_train_df.drop(['trafficSource_campaignCode'], axis=1, inplace=True)\n",
    "\n",
    "g_train_df['totals_transactionRevenue'] = g_train_df['totals_transactionRevenue'].astype(float)\n",
    "g_train_df['totals_transactionRevenue'] = g_train_df['totals_transactionRevenue'].fillna(0)\n",
    "\n",
    "g_train_df = process_format(g_train_df)\n",
    "g_tain_df = process_device(g_train_df)\n",
    "g_train_df = process_totals(g_train_df)\n",
    "g_train_df = process_geo_network(g_train_df)\n",
    "g_train_df = process_traffic_source(g_train_df)   \n",
    "\n",
    "g_test_df = process_format(g_test_df)\n",
    "g_test_df = process_device(g_test_df)\n",
    "g_test_df = process_totals(g_test_df)\n",
    "g_test_df = process_geo_network(g_test_df)\n",
    "g_test_df = process_traffic_source(g_test_df)   \n",
    "\n",
    "# Numeric Columns\n",
    "\n",
    "num_cols = ['month_unique_user_count','day_unique_user_count','weekday_unique_user_count','visitNumber','total_hits','total_pageviews','mean_hits_per_day','sum_hits_per_day','max_hits_per_day','min_hits_per_day','var_hits_per_day',\n",
    "           'mean_pageviews_per_day','sum_pageviews_per_day','max_pageviews_per_day','min_pageviews_per_day',\n",
    "           'sum_pageviews_per_network_domain','count_pageviews_per_network_domain','mean_pageviews_per_network_domain','sum_hits_per_network_domain',\n",
    "            'count_hits_per_network_domain','mean_hits_per_network_domain','medium_hits_mean',\n",
    "           'medium_hits_min','medium_hits_max','medium_hits_sum']\n",
    "\n",
    "not_used_cols = ['visitNumber','date','fullVisitorId','sessionId','visitId','visitStartTime',\n",
    "                'totals_transactionRevenue','trafficSource_referralPath']\n",
    "\n",
    "cat_cols = [col for col in g_train_df.columns if col not in num_cols and col not in not_used_cols]\n",
    "print(cat_cols)\n",
    "\n",
    "merged_df = pd.concat([g_train_df,g_test_df])\n",
    "ohe_columns = []\n",
    "for i in cat_cols:\n",
    "    if len(set(merged_df[i].values))<100:\n",
    "        ohe_columns.append(i)\n",
    "            \n",
    "print('ohe_cols:',ohe_columns)\n",
    "trn_shape=g_train_df.shape[0]\n",
    "merged_df = pd.get_dummies(merged_df,columns=ohe_columns)\n",
    "g_train_df = merged_df[:trn_shape]\n",
    "g_test_df  = merged_df[trn_shape:]\n",
    "g_train_df = g_train_df.loc[:,~g_train_df.columns.duplicated()]\n",
    "g_test_df = g_test_df.loc[:,~g_test_df.columns.duplicated()]\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in cat_cols:\n",
    "    if col in ohe_columns:\n",
    "        continue\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(g_train_df[col].values.astype('str')) + list(g_test_df[col].values.astype('str')))\n",
    "    g_train_df[col]= lbl.transform(list(g_train_df[col].values.astype('str')))\n",
    "    g_test_df[col]= lbl.transform(list(g_test_df[col].values.astype('str')))      \n",
    "    \n",
    "X = g_train_df.drop(not_used_cols,axis=1)\n",
    "y = g_train_df['totals_transactionRevenue']\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn import model_selection,preprocessing,metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FOLDs= StratifiedKFold(n_splits=5,shuffle=True,random_state=5)\n",
    "oof_lgb = np.zeros(len(g_train_df))\n",
    "predictions_lgb = np.zeros(len(g_test_df))                       \n",
    "features_lgb = list(X.columns)\n",
    "feature_importance_df_lgb = pd.DataFrame()\n",
    "                           \n",
    "for fold_, (trn_idx,val_idx) in enumerate(FOLDs.split(X,y_categorized)):\n",
    "    trn_data = lgb.Dataset(X.iloc[trn_idx],label=y_log.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(X.iloc[val_idx],label=y_log.iloc[val_idx])\n",
    "    num_round=2000\n",
    "    clf = lgb.train(lgb_params1,trn_data,num_round,valid_sets=[trn_data,val_data],verbose_eval=1000,early_stopping_rounds=100)\n",
    "    oof_lgb[val_idx] = clf.predict(X.iloc[val_idx],num_iteration=clf.best_iteration)\n",
    "    fold_importance_df_lgb = pd.DataFrame()\n",
    "    fold_importance_df_lgb[\"feature\"] = features_lgb\n",
    "    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb,fold_importance_df_lgb],axis=0)\n",
    "    predictions_lgb += clf.predict(X_test,num_iteration=clf.best_iteration) / FOLDs.n_splits\n",
    "    \n",
    "    \n",
    "cols = feature_importance_df_lgb[[\"feature\",\"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\",ascending=False)[:50].index\n",
    "best_features_lgb = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.barplot(x=\"importance\",y=\"feature\",data=best_features_lgb.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('LightGBM features( avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importancers.png')\n",
    "x = []\n",
    "for i in oof_lgb:\n",
    "    if i<0:\n",
    "        x.append(0.0)\n",
    "    else:\n",
    "        x.append(i)\n",
    "cv_lgb = mean_squared_error(x,y_log)**0.5\n",
    "cv_lgb = str(cv_lgb)\n",
    "cv_lgb = cv_lgb[:10]\n",
    "pd.DataFrame({'preds':x}).to_csv('lgb_oof_'+cv_lgb + '.csv',index=False)\n",
    "print(\"CV_LGB:\",cv_lgb)\n",
    "\n",
    "sub_df = g_test_df[['fullVisitorId']].copy()\n",
    "predictions_lgb[predictions_lgb<0] = 0\n",
    "sub_df['PredictedLogRevenue'] = np.expm1(predictions_lgb)\n",
    "sub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\n",
    "sub_df.columns = ['fullVisitorId',\"PredictedLogRevenue\"]\n",
    "\n",
    "sub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\n",
    "sub_df.to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
